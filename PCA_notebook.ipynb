{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "We will implement the PCA algorithm. We will first implement PCA, then apply it (once again) to the MNIST digit dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objective\n",
    "1. Write code that implements PCA.\n",
    "2. Write code that implements PCA for high-dimensional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the packages we need for this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE: DO NOT EDIT\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_mnist\n",
    "\n",
    "MNIST = load_mnist('./')\n",
    "images, labels = MNIST['data'], MNIST['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot a digit from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(images[0].reshape(28,28), cmap='gray');\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Now we will implement PCA. Before we do that, let's pause for a moment and\n",
    "think about the steps for performing PCA. Assume that we are performing PCA on\n",
    "some dataset $\\boldsymbol X$ for $M$ principal components. \n",
    "We then need to perform the following steps, which we break into parts:\n",
    "\n",
    "1. Data normalization (`normalize`).\n",
    "2. Find eigenvalues and corresponding eigenvectors for the covariance matrix $S$.\n",
    "   Sort by the largest eigenvalues and the corresponding eigenvectors (`eig`).\n",
    "3. Compute the orthogonal projection matrix and use that to project the data onto the subspace spanned by the eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization `normalize`\n",
    "\n",
    "We will first implement the data normalization mentioned above.\n",
    "\n",
    "Before we implement the main steps of PCA, we will need to do some data preprocessing.\n",
    "\n",
    "To preprocess the dataset for PCA, we will make sure that the dataset has zero mean. Given a dataset $\\mathbf{X}$, we will subtract the mean vector from each row of the dataset to obtain a zero-mean dataset $\\overline{\\mathbf{X}}$. In the first part of this notebook, you will implement `normalize` to do that.\n",
    "\n",
    "To work with images, it's also a common practice to convert the pixels from unsigned interger 8 (uint8) encoding to a floating point number representation between 0-1. We will do this conversion for you for the MNIST dataset so that you don't have to worry about it.\n",
    "\n",
    "Data normalization is a common practice. More details can be found in\n",
    "[Data Normalization or Feature Scaling](https://en.wikipedia.org/wiki/Feature_scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10a35b1c4186ecb4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def normalize(X):\n",
    "    \"\"\"Normalize the given dataset X to have zero mean.\n",
    "    Args:\n",
    "        X: ndarray, dataset of shape (N,D)\n",
    "    \n",
    "    Returns:\n",
    "        (Xbar, mean): tuple of ndarray, Xbar is the normalized dataset\n",
    "        with mean 0; mean is the sample mean of the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "        N, D = X.shape\n",
    "        mu = np.zeros((D,)) # <-- EDIT THIS, compute the mean of X\n",
    "        Xbar = X            # <-- EDIT THIS, compute the normalized data Xbar\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#     print('X:', X)\n",
    "#     print('----------')\n",
    "#     mu = np.mean(X, axis=0) # (D * 1) Matrix\n",
    "#     print('mu:', mu)\n",
    "#     print('----------')\n",
    "#     X = X - mu\n",
    "#     print('new X:', X)\n",
    "#     print('----------')\n",
    "    \"\"\"\n",
    "    #     x_bar = X / np.linalg.norm(X, axis=0, ord=0) # (N * D) matrix - wrong ord\n",
    "    #     x_bar = X / np.linalg.norm(X, axis=0) # (N * D) matrix #fails because of NAN\n",
    "    #     x_bar = X / np.linalg.norm(X) # works not intuitive\n",
    "    \"\"\"\n",
    "#     print(np.linalg.norm(X, axis=0), '***', np.zeros_like(X))\n",
    "#     x_bar = np.divide(X, np.linalg.norm(X, axis=0), out=np.zeros_like(X), where=np.linalg.norm(X, axis=0)!=0) #works\n",
    "#     print('x_bar:', x_bar)\n",
    "#     print('---------')\n",
    "#     print('mean of x_bar:', np.mean(x_bar, 0))\n",
    "#     print('----------')\n",
    "    mu = np.mean(X, axis=0) # (D * 1) Matrix\n",
    "    X = X - mu\n",
    "    x_bar = np.divide(X, np.linalg.norm(X, axis=0), out=np.zeros_like(X), where=np.linalg.norm(X, axis=0)!=0)\n",
    "    print(\"Norm:\", np.linalg.norm(X, axis=0))\n",
    "    return x_bar, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def normalize_new(X):\n",
    "    mu = np.mean(X, axis=0) # (D * 1) Matrix\n",
    "    X = X - mu\n",
    "    norm = np.linalg.norm(X, axis=0)\n",
    "    x_bar = np.divide(X, norm, out=np.zeros_like(X), where=np.linalg.norm(X, axis=0)!=0)\n",
    "#     print(\"Norm:\", norm)\n",
    "    return x_bar, mu, np.linalg.norm(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8ed3ab2f7f38aab4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: [1.41421356 1.41421356]\n",
      "Norm: [1. 1.]\n",
      "Norm: [1.41421356 0.        ]\n",
      "Norm: [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test data normalization\"\"\"\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "\n",
    "X0 = np.array([[0, 0.0], \n",
    "               [1.0, 1.0], \n",
    "               [2.0, 2.0]])\n",
    "X0_normalize, X0_mean = normalize(X0)\n",
    "# Test that normalized data has zero mean\n",
    "assert_allclose(np.mean(X0_normalize, 0), np.zeros((2,)))\n",
    "assert_allclose(X0_mean, np.array([1.0, 1.0]))\n",
    "# print('*************')\n",
    "# print('comparison:', normalize(X0_normalize)[0], X0_normalize)\n",
    "# print('*************')\n",
    "assert_allclose(normalize(X0_normalize)[0], X0_normalize)\n",
    "\n",
    "\n",
    "X0 = np.array([[0, 0.0], \n",
    "               [1.0, 0.0], \n",
    "               [2.0, 0.0]])\n",
    "X0_normalize, X0_mean = normalize(X0)\n",
    "# Test that normalized data has zero mean and unit variance\n",
    "assert_allclose(np.mean(X0_normalize, 0), np.zeros((2,)))\n",
    "assert_allclose(X0_mean, np.array([1.0, 0.0]))\n",
    "assert_allclose(normalize(X0_normalize)[0], X0_normalize)\n",
    "\n",
    "# Some hidden tests below\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute eigenvalues and eigenvectors `eig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d74b9253f3a0461",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def eig(S):\n",
    "    \"\"\"Compute the eigenvalues and corresponding eigenvectors\n",
    "        for the covariance matrix S.\n",
    "    Args:\n",
    "        S: ndarray, covariance matrix\n",
    "\n",
    "    Returns:\n",
    "        (eigvals, eigvecs): ndarray, the eigenvalues and eigenvectors\n",
    "\n",
    "    Note:\n",
    "        the eigenvals and eigenvecs should be sorted in descending\n",
    "        order of the eigen values\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Uncomment and modify the code below\n",
    "#     # Compute the eigenvalues and eigenvectors\n",
    "#     # You can use library routines in `np.linalg.*` \n",
    "#     # https://numpy.org/doc/stable/reference/routines.linalg.html\n",
    "#     # for this\n",
    "#     eigvals, eigvecs = None, None\n",
    "#     # The eigenvalues and eigenvectors need to be\n",
    "#     # sorted in descending order according to the eigenvalues\n",
    "#     # We will use `np.argsort` (https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html)\n",
    "#     # to find a permutation of the indices\n",
    "#     # of eigvals that will sort eigvals in ascending order and\n",
    "#     # then find the descending order via [::-1], which reverse\n",
    "#     # the indices\n",
    "#     sort_indices = np.argsort(eigvals)[::-1]\n",
    "#     # Notice that we are sorting the columns (not rows) of\n",
    "#     # eigvecs since the columns represent the eigenvectors.\n",
    "#     return eigvals[sort_indices], eigvecs[:, sort_indices]\n",
    "    eigvals, eigvecs = np.linalg.eig(S)\n",
    "    sort_indices = np.argsort(eigvals)[::-1]\n",
    "    return eigvals[sort_indices], eigvecs[:, sort_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some test cases for implementing `eig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a8db750754a119a2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def _flip_eigenvectors(B):\n",
    "    \"\"\"Flip the eigenvectors.    \n",
    "    \"\"\"\n",
    "    signs = np.sign(B[np.argmax(np.abs(B), axis=0), range(B.shape[1])])\n",
    "    return B * signs\n",
    "\n",
    "def _normalize_eigenvectors(B):\n",
    "    # Normalize eigenvectors to have unit length\n",
    "    # Also flip the direction of the eigenvector based on\n",
    "    # the first element\n",
    "    B_normalized = B / np.linalg.norm(B, axis=0)\n",
    "    for i in range(B.shape[1]):\n",
    "        if (B_normalized[0, i] < 0):\n",
    "            B_normalized[:, i] = -B_normalized[:, i]\n",
    "    return B_normalized\n",
    "\n",
    "\n",
    "A = np.array([[3, 2], [2, 3]])\n",
    "expected_eigenvalues = np.array([5., 1.])\n",
    "expected_eigenvectors = np.array(\n",
    "    [[ 0.70710678, -0.70710678],\n",
    "     [ 0.70710678,  0.70710678]]\n",
    ")\n",
    "actual_eigenvalues, actual_eigenvectors = eig(A)\n",
    "# Check that the eigenvalues match\n",
    "assert_allclose(actual_eigenvalues, expected_eigenvalues)\n",
    "# Check that the eigenvectors match\n",
    "assert_allclose(\n",
    "    _normalize_eigenvectors(actual_eigenvectors),\n",
    "    _normalize_eigenvectors(expected_eigenvectors),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute projection matrix\n",
    "\n",
    "Next given a orthonormal basis spanned by the eigenvectors,\n",
    "we will compute the projection matrix. This should be the same\n",
    "as what you have done last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3498a1f49501ed3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def projection_matrix(B):\n",
    "    \"\"\"Compute the projection matrix onto the space spanned by `B`\n",
    "    Args:\n",
    "        B: ndarray of dimension (D, M), the basis for the subspace\n",
    "    \n",
    "    Returns:\n",
    "        P: the projection matrix\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Uncomment and modify the code below\n",
    "#     return np.eye(B.shape[0]) # <-- EDIT THIS to compute the projection matrix\n",
    "    return np.matmul(np.matmul(B, np.linalg.inv(np.matmul(B.T, B))), B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-65a86398cff0c3ac",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "B = np.array([[1, 0],\n",
    "              [1, 1],\n",
    "              [1, 2]])\n",
    "\n",
    "assert_allclose(\n",
    "    projection_matrix(B), \n",
    "    np.array([[5,  2, -1],\n",
    "              [2,  2,  2],\n",
    "              [-1, 2,  5]]) / 6\n",
    ")\n",
    "\n",
    "# Some hidden tests below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98795bac9f7e1f82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def PCA(X, num_components):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of size (N, D), where D is the dimension of the data,\n",
    "           and N is the number of datapoints\n",
    "        num_components: the number of principal components to use.\n",
    "    Returns:\n",
    "        the reconstructed data, the sample mean of the X, principal values\n",
    "        and principal components\n",
    "    \"\"\"\n",
    "    X_normalized, mean, norm_val = normalize_new(X)\n",
    "    S = np.cov(X_normalized, rowvar=False)\n",
    "    eig_vals, eig_vecs = eig(S)\n",
    "    principal_vals, principal_components = eig_vals[:num_components], eig_vecs[:, :num_components]\n",
    "    X_normalized_new = X_normalized * norm_val\n",
    "#     reconst = np.matmul(X_normalized, projection_matrix(principal_components)) # 1\n",
    "#     reconst = np.matmul(np.matmul(X_normalized, principal_components), principal_components.T) # 2 (1 == 2) \n",
    "#     reconst = np.matmul(np.matmul(X_normalized_new, principal_components), principal_components.T) # 3\n",
    "    reconst = np.matmul(X_normalized_new, projection_matrix(principal_components)) # 4 (3 == 4)\n",
    "    reconst = reconst + mean\n",
    "    return reconst, mean, principal_vals, principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def PCA_debugger(X, num_components):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of size (N, D), where D is the dimension of the data,\n",
    "           and N is the number of datapoints\n",
    "        num_components: the number of principal components to use.\n",
    "    Returns:\n",
    "        the reconstructed data, the sample mean of the X, principal values\n",
    "        and principal components\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # your solution should take advantage of the functions you have implemented above.\n",
    "    ### Uncomment and modify the code below\n",
    "#     # first perform normalization on the digits so that they have zero mean and unit variance\n",
    "#     X_normalized, mean = None, None # EDIT THIS\n",
    "#     # Then compute the data covariance matrix S\n",
    "#     S = None # EDIT THIS\n",
    "\n",
    "#     # Next find eigenvalues and corresponding eigenvectors for S\n",
    "#     eig_vals, eig_vecs = eig(S)\n",
    "#     # Take the top `num_components` of eig_vals and eig_vecs,\n",
    "#     # This will be the corresponding principal values and components\n",
    "#     principal_vals, principal_components = None, None\n",
    "\n",
    "#     # reconstruct the data from the using the basis spanned by the principal components\n",
    "#     # Notice that we have subtracted the mean from X so make sure that you add it back\n",
    "#     # to the reconstructed data\n",
    "#     reconst = np.zeros_like(X_normalized)\n",
    "#     return reconst, mean, principal_vals, principal_components\n",
    "    X_normalized, mean, norm_val = normalize_new(X)\n",
    "    S = np.cov(X_normalized, rowvar=False)\n",
    "    eig_vals, eig_vecs = eig(S)\n",
    "    print(X_normalized.shape, eig_vecs.shape, norm_val.shape)\n",
    "    principal_vals, principal_components = eig_vals[:num_components], eig_vecs[:, :num_components]\n",
    "    print(principal_components.shape)\n",
    "    X_normalized_new = X_normalized * norm_val\n",
    "#     reconst = np.matmul(X_normalized, projection_matrix(principal_components)) # 1\n",
    "#     reconst = np.matmul(np.matmul(X_normalized, principal_components), principal_components.T) # 2 (1 == 2) \n",
    "#     reconst = np.matmul(np.matmul(X_normalized_new, principal_components), principal_components.T) # 3\n",
    "    reconst = np.matmul(X_normalized_new, projection_matrix(principal_components)) # 4 (3 == 4)\n",
    "#     print(projection_matrix(principal_components))\n",
    "    print(projection_matrix(principal_components).shape)\n",
    "    reconst = reconst + mean\n",
    "    print(reconst.shape)\n",
    "    print(mean)\n",
    "    print('Reconst:', reconst)\n",
    "    return reconst, mean, principal_vals, principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None, label=None):\n",
    "    \"\"\"Draw a vector from v0 to v1.\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, \n",
    "                    color='k')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some test cases that check the implementation of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-90d1f11031fdf7d8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "D = 2\n",
    "N = 10\n",
    "# Generate a dataset X from a 2D Gaussian distribution\n",
    "mvn = scipy.stats.multivariate_normal(\n",
    "    mean=np.ones(D, dtype=np.float64), \n",
    "    cov=np.array([[1, 0.8], [0.8, 1]], dtype=np.float64)\n",
    ")\n",
    "\n",
    "X = mvn.rvs((N,), random_state=np.random.RandomState(0))\n",
    "reconst, m, pv, pc = PCA(X, 1)\n",
    "# Check the shape returned by the PCA implementation matches the specification.\n",
    "assert reconst.shape == X.shape\n",
    "assert m.shape == (D, )\n",
    "assert pv.shape == (1, )\n",
    "assert pc.shape == (D, 1)\n",
    "\n",
    "# Check that PCA with num_components == D gives identical reconstruction\n",
    "reconst, m, pv, pc = PCA(X, D)\n",
    "assert reconst.shape == X.shape\n",
    "assert m.shape == (D, )\n",
    "assert pv.shape == (2, )\n",
    "assert pc.shape == (D, 2)\n",
    "assert_allclose(reconst, X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize PCA\n",
    "We will first visualize what PCA does on a 2D toy dataset. You can use the visualization\n",
    "below to get better intuition about what PCA does and use it to debug your code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mvn = scipy.stats.multivariate_normal(\n",
    "    mean=np.ones(2), \n",
    "    cov=np.array([[1, 0.8], [0.8, 1]])\n",
    ")\n",
    "\n",
    "X = mvn.rvs((100,), random_state=np.random.RandomState(0))\n",
    "\n",
    "num_components = 1\n",
    "X_reconst, mean, principal_values, principal_components = PCA(X, num_components)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# eig_vals, eig_vecs = eig_vals[:num_components], eig_vecs[:, :num_components]\n",
    "ax.scatter(X[:, 0], X[:, 1], label='data')\n",
    "for (princial_variance, principal_component) in (zip(principal_values, principal_components.T)):\n",
    "    draw_vector(\n",
    "        mean, mean + np.sqrt(princial_variance) * principal_component, \n",
    "        ax=ax)\n",
    "ax.scatter(X_reconst[:, 0], X_reconst[:, 1], label='reconstructed')\n",
    "plt.axis('equal');\n",
    "plt.legend();\n",
    "ax.set(xlabel='$\\mathbf{x}_0$', ylabel='$\\mathbf{x}_1$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare our PCA implementation with the implementation in scikit-learn (a popular machine learning library in Python that includes implementation of PCA)\n",
    "to see\n",
    "if we get identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference in reconstruction for num_components = 1: 35.653348822016746\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-07, atol=0\n\nMismatched elements: 50 / 50 (100%)\nMax absolute difference: 2.56613136\nMax relative difference: 45.60438494\n x: array([[-0.026732,  1.274743,  1.222584,  0.071311,  1.060435],\n       [-0.397068,  0.380692,  0.129137,  0.134068,  0.414286],\n       [-0.125711,  1.035792,  0.930341,  0.088084,  0.887741],...\n y: array([[ 0.014741,  0.160996, -0.027409,  2.637442,  1.249575],\n       [-0.443697,  0.423012,  0.166586, -0.18637 ,  0.318027],\n       [-0.400097,  0.398092,  0.148135,  0.082193,  0.406623],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-441e0ab9694c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         )\n\u001b[1;32m     16\u001b[0m     )\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msklearn_reconst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_allclose\u001b[0;34m(actual, desired, rtol, atol, equal_nan, err_msg, verbose)\u001b[0m\n\u001b[1;32m   1531\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Not equal to tolerance rtol=%g, atol=%g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\n\u001b[0;32m-> 1533\u001b[0;31m                          verbose=verbose, header=header, equal_nan=equal_nan)\n\u001b[0m\u001b[1;32m   1534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    844\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=1e-07, atol=0\n\nMismatched elements: 50 / 50 (100%)\nMax absolute difference: 2.56613136\nMax relative difference: 45.60438494\n x: array([[-0.026732,  1.274743,  1.222584,  0.071311,  1.060435],\n       [-0.397068,  0.380692,  0.129137,  0.134068,  0.414286],\n       [-0.125711,  1.035792,  0.930341,  0.088084,  0.887741],...\n y: array([[ 0.014741,  0.160996, -0.027409,  2.637442,  1.249575],\n       [-0.443697,  0.423012,  0.166586, -0.18637 ,  0.318027],\n       [-0.400097,  0.398092,  0.148135,  0.082193,  0.406623],..."
     ]
    }
   ],
   "source": [
    "random = np.random.RandomState(0)\n",
    "X = random.randn(10, 5)\n",
    "\n",
    "from sklearn.decomposition import PCA as SKPCA\n",
    "\n",
    "for num_component in range(1, 4):\n",
    "    # We can compute a standard solution given by scikit-learn's implementation of PCA\n",
    "    pca = SKPCA(n_components=num_component, svd_solver=\"full\")\n",
    "    sklearn_reconst = pca.inverse_transform(pca.fit_transform(X))\n",
    "    reconst, _, _, _ = PCA(X, num_component)\n",
    "    # The difference in the result should be very small (<10^-20)\n",
    "    print(\n",
    "        \"difference in reconstruction for num_components = {}: {}\".format(\n",
    "            num_component, np.square(reconst - sklearn_reconst).sum()\n",
    "        )\n",
    "    )\n",
    "    np.testing.assert_allclose(reconst, sklearn_reconst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for MNIST digits\n",
    "\n",
    "Once you have implemented PCA correctly, it's time to apply to the MNIST dataset.\n",
    "First, we will do some preprocessing of the data to get it into a good shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-55fb94072c74b006",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Some preprocessing of the data\n",
    "NUM_DATAPOINTS = 1000\n",
    "X = (images.reshape(-1, 28 * 28)[:NUM_DATAPOINTS]) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst, _, _, _ = PCA(X, num_components=10)\n",
    "num_images_to_show = 10\n",
    "reconst_images = np.reshape(reconst[:num_images_to_show], (-1, 28, 28))\n",
    "fig, ax = plt.subplots(2, 1, figsize=(num_images_to_show * 3, 3))\n",
    "ax[0].imshow(np.concatenate(np.reshape(X[:num_images_to_show], (-1, 28, 28)), -1), cmap=\"gray\")\n",
    "ax[1].imshow(np.concatenate(reconst_images, -1), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greater number of of principal components we use, the smaller will our reconstruction\n",
    "error be. Now, let's answer the following question: \n",
    "\n",
    "\n",
    "> How many principal components do we need\n",
    "> in order to reach a Mean Squared Error (MSE) of less than $10.0$ for our dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a function in the next cell which computes the mean squared error (MSE), which will be useful for answering the question above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predict, actual):\n",
    "    \"\"\"Helper function for computing the mean squared error (MSE)\"\"\"\n",
    "    return np.square(predict - actual).sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "reconstructions = []\n",
    "# iterate over different number of principal components, and compute the MSE\n",
    "for num_component in range(1, 100, 5):\n",
    "    reconst, _, _, _ = PCA(X, num_component)\n",
    "    error = mse(reconst, X)\n",
    "    reconstructions.append(reconst)\n",
    "    # print('n = {:d}, reconstruction_error = {:f}'.format(num_component, error))\n",
    "    loss.append((num_component, error))\n",
    "\n",
    "reconstructions = np.asarray(reconstructions)\n",
    "reconstructions = reconstructions\n",
    "loss = np.asarray(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# create a table showing the number of principal components and MSE\n",
    "pd.DataFrame(loss, columns=['no. of components', 'mse']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also put these numbers into perspective by plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss[:,0], loss[:,1]);\n",
    "ax.axhline(10, linestyle='--', color='r', linewidth=2)\n",
    "ax.xaxis.set_ticks(np.arange(1, 100, 5));\n",
    "ax.set(xlabel='num_components', ylabel='MSE', title='MSE vs number of principal components');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But _numbers dont't tell us everything_! Just what does it mean _qualitatively_ for the loss to decrease from around\n",
    "$45.0$ to less than $10.0$?\n",
    "\n",
    "Let's find out! In the next cell, we draw the the leftmost image is the original dight. Then we show the reconstruction of the image on the right, in descending number of principal components used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(image_idx=(0, 1000))\n",
    "def show_num_components_reconst(image_idx):\n",
    "    fig, ax = plt.subplots(figsize=(20., 20.))\n",
    "    actual = X[image_idx]\n",
    "    # concatenate the actual and reconstructed images as large image before plotting it\n",
    "    x = np.concatenate([actual[np.newaxis, :], reconstructions[:, image_idx]])\n",
    "    ax.imshow(np.hstack(x.reshape(-1, 28, 28)[np.arange(10)]),\n",
    "              cmap='gray');\n",
    "    ax.axvline(28, color='orange', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also browse through the reconstructions for other digits. Once again, `interact` becomes handy for visualing the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(i=(0, 10))\n",
    "def show_pca_digits(i=1):\n",
    "    \"\"\"Show the i th digit and its reconstruction\"\"\"\n",
    "    plt.figure(figsize=(4,4))\n",
    "    actual_sample = X[i].reshape(28,28)\n",
    "    reconst_sample = (reconst[i, :]).reshape(28, 28)\n",
    "    plt.imshow(np.hstack([actual_sample, reconst_sample]), cmap='gray')\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for high-dimensional datasets\n",
    "\n",
    "Sometimes, the dimensionality of our dataset may be larger than the number of samples we\n",
    "have. Then it might be inefficient to perform PCA with your implementation above. Instead,\n",
    "as mentioned in the lectures, you can implement PCA in a more efficient manner, which we\n",
    "call \"PCA for high dimensional data\" (PCA_high_dim).\n",
    "\n",
    "Below are the steps for performing PCA for high dimensional dataset\n",
    "1. Normalize the dataset matrix $X$ to obtain $\\overline{X}$ that has zero mean.\n",
    "2. Compute the matrix $\\overline{X}\\overline{X}^T$ (a $N$ by $N$ matrix with $N << D$)\n",
    "3. Compute eigenvalues $\\lambda$s and eigenvectors $V$ for $\\overline{X}\\overline{X}^T$ with shape (N, N). Compare this with computing the eigenspectrum of $\\overline{X}^T\\overline{X}$ which has shape (D, D), when $N << D$, computation of the eigenspectrum of $\\overline{X}\\overline{X}^T$ will be computationally less expensive.\n",
    "4. Compute the eigenvectors for the original covariance matrix as $\\overline{X}^TV$. Choose the eigenvectors associated with the `n` largest eigenvalues to be the basis of the principal subspace $U$.\n",
    "    1. Notice that $\\overline{X}^TV$ would give a matrix of shape (D, N) but the eigenvectors beyond the Dth column will have eigenvalues of 0, so it is safe to drop any columns beyond the D'th dimension. \n",
    "    2. Also note that the columns of $U$ will not be unit-length if we pre-multiply $V$ with $\\overline{X}^T$, so we will have to normalize the columns of $U$ so that they have unit-length to be consistent with the `PCA` implementation above.\n",
    "5. Compute the orthogonal projection of the data onto the subspace spanned by columns of $U$. \n",
    "\n",
    "Functions you wrote for earlier assignments will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b8c115e4c40bd67",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def PCA_high_dim(X, num_components):\n",
    "    \"\"\"Compute PCA for small sample size but high-dimensional features. \n",
    "    Args:\n",
    "        X: ndarray of size (N, D), where D is the dimension of the sample,\n",
    "           and N is the number of samples\n",
    "        num_components: the number of principal components to use.\n",
    "    Returns:\n",
    "        X_reconstruct: (N, D) ndarray. the reconstruction\n",
    "        of X from the first `num_components` pricipal components.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Uncomment and modify the code below\n",
    "#     N, D = X.shape\n",
    "#     # Normalize the dataset\n",
    "#     X_normalized, mean = None, None\n",
    "#     # Find the covariance matrix\n",
    "#     M = np.dot(X_normalized, X_normalized.T) / N\n",
    "#     # Next find eigenvalues and corresponding eigenvectors for S\n",
    "#     # Make sure that you only take the first D eigenvalues/vectors\n",
    "#     # You can also take a look at the eigenvalues beyond column (D-1) and they should be \n",
    "#     # zero (or a very small number due to finite floating point precision)\n",
    "#     eig_vals, eig_vecs = None, None\n",
    "#     # Compute the eigenvalues and eigenvectors for the original system\n",
    "    \n",
    "#     # Normalize the eigenvectors to have unit-length\n",
    "#     # Take the top `num_components` of the eigenvalues / eigenvectors\n",
    "#     # as the principal values and principal components\n",
    "#     principal_values = None\n",
    "#     principal_components = None\n",
    "#     # reconstruct the images from the lower dimensional representation\n",
    "#     # Remember to add back the sample mean\n",
    "#     reconst = None\n",
    "#     return reconst, mean, principal_values, principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ddfc3a4390a8b957",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8334eae5a3ead42f",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Some hidden tests below\n",
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the same dataset, `PCA_high_dim` and `PCA` should give the same output. \n",
    "Assuming we have implemented `PCA`, correctly, we can then use `PCA` to test the correctness\n",
    "of `PCA_high_dim`. Given the same dataset, `PCA` and `PCA_high_dim` should give identical results.\n",
    "\n",
    "We can use this __invariant__\n",
    "to test our implementation of PCA_high_dim, assuming that we have correctly implemented `PCA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e1f35e6bd20a7b6a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "random = np.random.RandomState(0)\n",
    "# Generate some random data\n",
    "X = random.randn(5, 4)\n",
    "pca_rec, pca_mean, pca_pvs, pca_pcs = PCA(X, 2)\n",
    "pca_hd_rec, pca_hd_mean, pca_hd_pvs, pca_hd_pcs = PCA_high_dim(X, 2)\n",
    "# Check that the results returned by PCA and PCA_high_dim are identical\n",
    "np.testing.assert_allclose(pca_rec, pca_hd_rec)\n",
    "np.testing.assert_allclose(pca_mean, pca_hd_mean)\n",
    "np.testing.assert_allclose(pca_pvs, pca_pvs)\n",
    "np.testing.assert_allclose(pca_pcs, pca_pcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Congratulations_! You have now learned how PCA works!"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "mathematics-machine-learning-pca",
   "graded_item_id": "CXC11",
   "launcher_item_id": "ub5A7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
